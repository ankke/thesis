{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "from functools import partial\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import shutil\n",
    "from argparse import ArgumentParser\n",
    "from monai.handlers import EarlyStopHandler\n",
    "import torch\n",
    "from monai.data import DataLoader\n",
    "from data.dataset_mixed import build_mixed_data\n",
    "from data.dataset_road_network import build_road_network_data\n",
    "from data.dataset_synthetic_eye_vessels import build_synthetic_vessel_network_data\n",
    "from data.dataset_real_eye_vessels import build_real_vessel_network_data\n",
    "from training.evaluator import build_evaluator\n",
    "from training.trainer import build_trainer\n",
    "from models import build_model\n",
    "from utils.utils import image_graph_collate_road_network\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models.matcher import build_matcher\n",
    "from training.losses import SetCriterion\n",
    "from ignite.contrib.handlers.tqdm_logger import ProgressBar\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument('--config',\n",
    "                    default=None,\n",
    "                    help='config file (.yml) containing the hyper-parameters for training. '\n",
    "                         'If None, use the nnU-Net config. See /config for examples.')\n",
    "parser.add_argument('--resume', default=None,\n",
    "                    help='checkpoint of the last epoch of the model')\n",
    "parser.add_argument('--restore_state', dest='restore_state', help='whether the state should be restored', action='store_true')\n",
    "parser.add_argument('--seg_net', default=None,\n",
    "                    help='checkpoint of the segmentation model')\n",
    "parser.add_argument('--device', default='cuda',\n",
    "                    help='device to use for training')\n",
    "parser.add_argument('--cuda_visible_device', nargs='*', type=int, default=None,\n",
    "                    help='list of index where skip conn will be made')\n",
    "parser.add_argument('--recover_optim', default=False, action=\"store_true\",\n",
    "                    help=\"Whether to restore optimizer's state. Only necessary when resuming training.\")\n",
    "parser.add_argument('--exp_name', dest='exp_name', help='name of the experiment', type=str,required=True)\n",
    "parser.add_argument('--pretrain_seg', default=False, action=\"store_true\",\n",
    "                    help=\"Whether to pretrain on segs instead of raw images\")\n",
    "parser.add_argument('--no_strict_loading', default=False, action=\"store_true\",\n",
    "                    help=\"Whether the model was pretrained with domain adversarial. If true, the checkpoint will be loaded with strict=false\")\n",
    "parser.add_argument('--sspt', default=False, action=\"store_true\",\n",
    "                    help=\"Whether the model was pretrained with self supervised pretraining. If true, the checkpoint will be loaded accordingly. Only combine with resume.\")\n",
    "\n",
    "\n",
    "class obj:\n",
    "    def __init__(self, dict1):\n",
    "        self.__dict__.update(dict1)\n",
    "\n",
    "\n",
    "def dict2obj(dict1):\n",
    "    return json.loads(json.dumps(dict1), object_hook=obj)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    # Load the config files\n",
    "    with open(args.config) as f:\n",
    "        print('\\n*** Config file')\n",
    "        print(args.config)\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        print(args.exp_name)\n",
    "    config = dict2obj(config)\n",
    "    config.log.exp_name = args.exp_name\n",
    "\n",
    "    if args.cuda_visible_device:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(\n",
    "            map(str, args.cuda_visible_device))\n",
    "        print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "    exp_path = os.path.join(config.TRAIN.SAVE_PATH, \"runs\", '%s_%d' % (\n",
    "        config.log.exp_name, config.DATA.SEED))\n",
    "    if os.path.exists(exp_path):\n",
    "        print('ERROR: Experiment folder exist, please change exp name in config file')\n",
    "    else:\n",
    "        try:\n",
    "            os.makedirs(exp_path)\n",
    "            shutil.copyfile(args.config, os.path.join(exp_path, \"config.yaml\"))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "    device = torch.device(\"cuda\") if args.device == 'cuda' else torch.device(\"cpu\")\n",
    "\n",
    "    if config.DATA.DATASET == 'road_dataset':\n",
    "        build_dataset_function = build_road_network_data\n",
    "        config.DATA.MIXED = False\n",
    "    elif config.DATA.DATASET == 'synthetic_eye_vessel_dataset':\n",
    "        build_dataset_function = build_synthetic_vessel_network_data\n",
    "        config.DATA.MIXED = False\n",
    "    elif config.DATA.DATASET == 'real_eye_vessel_dataset':\n",
    "        build_dataset_function = build_real_vessel_network_data\n",
    "        config.DATA.MIXED = False\n",
    "    elif config.DATA.DATASET == 'mixed_road_dataset' or config.DATA.DATASET == 'mixed_synthetic_eye_vessel_dataset' or config.DATA.DATASET == \"mixed_real_eye_vessel_dataset\":\n",
    "        build_dataset_function = partial(build_mixed_data, upsample_target_domain=config.TRAIN.UPSAMPLE_TARGET_DOMAIN)\n",
    "        config.DATA.MIXED = True\n",
    "\n",
    "    train_ds, val_ds, sampler = build_dataset_function(\n",
    "        config, mode='split', use_grayscale=args.pretrain_seg, max_samples=config.DATA.NUM_SOURCE_SAMPLES, split=0.8\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds,\n",
    "                              batch_size=config.DATA.BATCH_SIZE,\n",
    "                              shuffle=not sampler,\n",
    "                              num_workers=config.DATA.NUM_WORKERS,\n",
    "                              collate_fn=image_graph_collate_road_network,\n",
    "                              pin_memory=True,\n",
    "                              sampler=sampler)\n",
    "\n",
    "    val_loader = DataLoader(val_ds,\n",
    "                            batch_size=config.DATA.BATCH_SIZE,\n",
    "                            shuffle=False,\n",
    "                            num_workers=config.DATA.NUM_WORKERS,\n",
    "                            collate_fn=image_graph_collate_road_network,\n",
    "                            pin_memory=True)\n",
    "\n",
    "    net = build_model(config).to(device)\n",
    "\n",
    "    seg_net = build_model(config).to(device)\n",
    "\n",
    "    matcher = build_matcher(config)\n",
    "    loss = SetCriterion(\n",
    "        config,\n",
    "        matcher,\n",
    "        net,\n",
    "        num_edge_samples=config.TRAIN.NUM_EDGE_SAMPLES,\n",
    "        edge_upsampling=config.TRAIN.EDGE_UPSAMPLING,\n",
    "        domain_class_weight=torch.tensor(config.TRAIN.DOMAIN_WEIGHTING, device=device)\n",
    "    )\n",
    "    val_loss = SetCriterion(config, matcher, net, num_edge_samples=9999, edge_upsampling=False)\n",
    "\n",
    "    param_dicts = [\n",
    "        {\n",
    "            \"params\":\n",
    "                [p for n, p in net.named_parameters()\n",
    "                 if not match_name_keywords(n, [\"encoder.0\"]) and not match_name_keywords(n, ['reference_points', 'sampling_offsets']) and not match_name_keywords(n, [\"domain_discriminator\"]) and p.requires_grad],\n",
    "            \"lr\": float(config.TRAIN.LR),\n",
    "            \"weight_decay\": float(config.TRAIN.WEIGHT_DECAY)\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in net.named_parameters() if match_name_keywords(n, [\"encoder.0\"]) and p.requires_grad],\n",
    "            \"lr\": float(config.TRAIN.LR_BACKBONE),\n",
    "            \"weight_decay\": float(config.TRAIN.WEIGHT_DECAY)\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in net.named_parameters() if match_name_keywords(n, ['reference_points', 'sampling_offsets']) and p.requires_grad],\n",
    "            \"lr\": float(config.TRAIN.LR)*0.1,\n",
    "            \"weight_decay\": float(config.TRAIN.WEIGHT_DECAY)\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in net.named_parameters() if match_name_keywords(n, ['domain_discriminator']) and p.requires_grad],\n",
    "            \"lr\": float(config.TRAIN.LR_DOMAIN),\n",
    "            \"weight_decay\": float(config.TRAIN.WEIGHT_DECAY)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        param_dicts, lr=float(config.TRAIN.LR), weight_decay=float(config.TRAIN.WEIGHT_DECAY)\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, config.TRAIN.LR_DROP)\n",
    "\n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "\n",
    "        if args.sspt:\n",
    "            checkpoint['state_dict'] = {k[17:]: v for k, v in checkpoint['state_dict'].items() if k.startswith(\"momentum_encoder\")}\n",
    "            net.load_state_dict(checkpoint['state_dict'], strict=True)\n",
    "        else:\n",
    "            net.load_state_dict(checkpoint['net'], strict=not args.no_strict_loading)\n",
    "            if args.recover_optim:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            if args.restore_state:\n",
    "                scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "                last_epoch = scheduler.last_epoch\n",
    "                scheduler.step_size = config.TRAIN.LR_DROP\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f'lr: {param_group[\"lr\"]}, number of params: {len(param_group[\"params\"])}')\n",
    "\n",
    "    if args.seg_net:\n",
    "        checkpoint = torch.load(args.seg_net, map_location='cpu')\n",
    "        seg_net.load_state_dict(checkpoint['net'])\n",
    "        # net.load_state_dict(checkpoint['net'])\n",
    "    #     # 1. filter out unnecessary keys\n",
    "    #     pretrained_dict = {k: v for k, v in checkpoint.items() if match_name_keywords(k, [\"encoder.0\"])}\n",
    "    #     # 3. load the new state dict\n",
    "    #     net.load_state_dict(pretrained_dict, strict=False)\n",
    "    #     # net.load_state_dict(checkpoint['net'], strict=False)\n",
    "    #     # for param in seg_net.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "    writer = SummaryWriter(\n",
    "        log_dir=os.path.join(config.TRAIN.SAVE_PATH, \"runs\", '%s_%d' % (\n",
    "            config.log.exp_name, config.DATA.SEED)),\n",
    "    )\n",
    "\n",
    "    \"\"\"early_stop_handler = EarlyStopHandler(\n",
    "            patience=15,\n",
    "            score_function=lambda x: -x.state.output[\"loss\"][\"total\"].item()\n",
    "    )\"\"\"\n",
    "\n",
    "    evaluator = build_evaluator(\n",
    "        val_loader,\n",
    "        net,\n",
    "        val_loss,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        writer,\n",
    "        config,\n",
    "        device,\n",
    "        early_stop_handler\n",
    "    )\n",
    "    trainer = build_trainer(\n",
    "        train_loader,\n",
    "        net,\n",
    "        seg_net,\n",
    "        loss,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        writer,\n",
    "        evaluator,\n",
    "        config,\n",
    "        device,\n",
    "        # fp16=args.fp16,\n",
    "    )\n",
    "\n",
    "    early_stop_handler.set_trainer(trainer)\n",
    "\n",
    "    if args.resume and args.restore_state:\n",
    "        evaluator.state.epoch = last_epoch\n",
    "        trainer.state.epoch = last_epoch\n",
    "        trainer.state.iteration = trainer.state.epoch_length * last_epoch\n",
    "\n",
    "    pbar = ProgressBar()\n",
    "    pbar.attach(trainer, output_transform=lambda x: {\n",
    "                'loss': x[\"loss\"][\"total\"].item()})\n",
    "#     logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    print(torch.cuda.is_available(), torch.backends.cudnn.is_available())\n",
    "    trainer.run()\n",
    "\n",
    "\n",
    "def match_name_keywords(n, name_keywords):\n",
    "    out = False\n",
    "    for b in name_keywords:\n",
    "        if b in n:\n",
    "            out = True\n",
    "            break\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_args = ['--config', 'configs/real_config.yaml', '--cuda_visible_device', '0', '--exp_name', 'octa_greed_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(cmd_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Config file\n",
      "configs/real_config.yaml\n",
      "octa_greed_1\n",
      "0\n",
      "ERROR: Experiment folder exist, please change exp name in config file\n",
      "<__main__.obj object at 0x7f34c074b9a0>\n",
      "<__main__.obj object at 0x7f34c074b9a0>\n",
      "lr: 0.0002, number of params: 176\n",
      "lr: 2e-05, number of params: 93\n",
      "lr: 2e-05, number of params: 20\n",
      "lr: 0.0002, number of params: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'early_stop_handler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [1], line 221\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    202\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(\n\u001b[1;32m    203\u001b[0m     log_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39mTRAIN\u001b[38;5;241m.\u001b[39mSAVE_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    204\u001b[0m         config\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mexp_name, config\u001b[38;5;241m.\u001b[39mDATA\u001b[38;5;241m.\u001b[39mSEED)),\n\u001b[1;32m    205\u001b[0m )\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m\"\"\"early_stop_handler = EarlyStopHandler(\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m        patience=15,\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m        score_function=lambda x: -x.state.output[\"loss\"][\"total\"].item()\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m)\"\"\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m build_evaluator(\n\u001b[1;32m    213\u001b[0m     val_loader,\n\u001b[1;32m    214\u001b[0m     net,\n\u001b[1;32m    215\u001b[0m     val_loss,\n\u001b[1;32m    216\u001b[0m     optimizer,\n\u001b[1;32m    217\u001b[0m     scheduler,\n\u001b[1;32m    218\u001b[0m     writer,\n\u001b[1;32m    219\u001b[0m     config,\n\u001b[1;32m    220\u001b[0m     device,\n\u001b[0;32m--> 221\u001b[0m     \u001b[43mearly_stop_handler\u001b[49m\n\u001b[1;32m    222\u001b[0m )\n\u001b[1;32m    223\u001b[0m trainer \u001b[38;5;241m=\u001b[39m build_trainer(\n\u001b[1;32m    224\u001b[0m     train_loader,\n\u001b[1;32m    225\u001b[0m     net,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# fp16=args.fp16,\u001b[39;00m\n\u001b[1;32m    235\u001b[0m )\n\u001b[1;32m    237\u001b[0m early_stop_handler\u001b[38;5;241m.\u001b[39mset_trainer(trainer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'early_stop_handler' is not defined"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rf",
   "language": "python",
   "name": "rf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
